{
 "metadata": {
  "name": "",
  "signature": "sha256:2752fca01f10dd30181a915285f8844a7b312e0773e446cff5deecb6e7b3cc9a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Analysis I - homework - week 11 {-}"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**1.** $(a)$ Suppose that $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ is of class $C^1$ and $Df (x_0 )$ has rank $m$. This means that $Df (x_0 )$ as a linear map is onto. Then show that there is a neighborhood of $f(x_0)$ lying in the image of $f$.\n",
      "\n",
      "$(b)$ Suppose $f : \\mathbb{R}^n \\to \\mathbb{R}^m$ is $C^1$ and $Df (x_0 )$ is one-to-one. Show $f$ is one-to-one on a neighborhood of $x_0$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** *Lemma.* (Generalization of Straightening-out Theorem) Let $f: \\mathbb{R}^n \\to \\mathbb{R}^m$, $n\\ge m$, $f \\in C^1$. Suppose $f(x_0) = 0$ and $Df(0)$ has rank $m$. Then there is an open set $U$, an open set $V$ containing $x_0$, and a function $h:U\\to V$ of class $C^1$, with inverse $h^{-1}:V\\to U$ of class $C^1$, such that for all $(x_1,\\ldots,x_n)\\in U$, we have\n",
      "$$f(h(x_1,\\ldots,x_n)) = (x_{n-m+1},\\ldots,x_n).$$\n",
      "\n",
      "For a proof of this lemma, see Exercise 7.3 in the textbook and imitate the proof of Straightening-out Theorem without any much modification (replace $\\mathbb{R}^{n-1}\\times \\mathbb{R}$ by $\\mathbb{R}^{n-m}\\times \\mathbb{R}^m$).\n",
      "\n",
      "$(a)$ By $Df(x_0)$ has rank $m$, we have $n \\ge m$. Put $g(x) = f(x) - f(x_0)$, so $g(x_0) = 0$. Moreover, $Dg(x_0) = Df(x_0)$, so $Dg(x_0)$ has rank $m$. By the above lemma, there is an open set $U$, an open set $V$ containing $x_0$, and a surjective function $h:U\\to V$ such that for all $(x_1,\\ldots,x_n)\\in U$, we have\n",
      "$$g(h(x_1,\\ldots,x_n)) = (x_{n-m+1},\\ldots,x_n).$$\n",
      "\n",
      "We have $g(V) = (g\\circ h) (U)$, which is open because $g\\circ h$ is a projection. By $x_0 \\in V$, we conclude $g(V)$ is a neighborhood of $0$, hence $f(V)\\subset \\operatorname{Im}(f)$ is a neighborhood of $f(x_0)$.\n",
      "\n",
      "$(b)$ In Linear Algebra, we know that if $Df(x_0)$ is one-to-one then $Df(x_0)$ has rank $n$ (by $n = \\operatorname{rank}Df(x_0) + \\dim \\operatorname{Ker}( Df(x_0))$). So $n\\le m$. Apply Theorem 7.4, there are open sets $U$ and $V$ in $\\mathbb{R}^m$ with $f(x_0)\\in U$ and a function $g:U\\to V$ such that for all $(x_1,\\ldots,x_n)\\in f^{-1}(U)$, we have\n",
      "$$g\\circ  f(x_1,\\ldots,x_n) = (x_1,\\ldots,x_n,0,\\ldots,0).$$\n",
      "\n",
      "By $g\\circ f$ is one-to-one on $f^{-1}(U)$, $f$ must be one-to-one on $f^{-1}(U)$, which is a neighborhood of $x_0$. $\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**2.** Does the function $h$ in Theorem 3 (*Straightening-out Theorem*) have to be unique? Discuss."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** No. For example, let $f:\\mathbb{R}^2 \\to \\mathbb{R}^2$ be defined by $f(x,y) = x+y$. Clearly, $f \\in C^1$ (indeed $f\\in C^{\\infty}$) and $Df(0,0) \\ne 0$. Consider $h_1,h_2 :\\mathbb{R}^2 \\to \\mathbb{R}^2$ be defined by $h_1(x,y) = (x,y -x)$, $h_2(x,y) = (x+1,y-x-1)$. We have $h_1^{-1}(x,y) = (x,x+y)$ and $h_2^{-1}(x,y) = (x-1, x + y )$, so all $h_1, h_2, h_1^{-1}, h_2^{-1}$ are in $C^{1}$. Moreover, $f(h_1(x,y)) = f(h_2(x,y)) = y$. We get two functions satisfy the conclusion of Theorem 3. $\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**3.** Let $U \\subset \\mathbb{R}^n$ be open. We say the boundary $\\partial U$ is $C^p$ if for each point $x_0 \\in \\partial U$ there exist $r > 0$ and a $C^p$ function $\\gamma : \\mathbb{R}^{n-1} \\to \\mathbb{R}$ such that - upon relabeling and reorienting the coordinates axes if necessary - we have\n",
      "$$U \\cap B(x_0 , r) = \\{x \\in B(x_0 , r) \\mid x_n > \\gamma(x_1 , \\ldots , x_{n\u22121})\\}.$$\n",
      "\n",
      "Likewise, $\\partial U$ is $C^{\\infty}$ if $\\gamma$ is $C^{\\infty}$ function.\n",
      "\n",
      "Determine that the following boundaries are $C^1$:\n",
      "\n",
      "$(a)$ $U := \\{(x, y, z) \\in \\mathbb{R}^3 \\mid x + y + z < 0\\}$ and its boundary is $\\partial U := \\{(x, y, z) \\in \\mathbb{R}^3 \\mid x + y + z = 0\\}$.\n",
      "\n",
      "$(b)$ $U := \\{(x, y) \\in \\mathbb{R}^2 \\mid  x^3 + y^3 < 1\\}$ and its boundary is $\\partial U := \\{(x, y) \\in \\mathbb{R}^2 \\mid  x^3 + y^3 = 1\\}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** $(a)$ For $(x_0,y_0,z_0) \\in \\partial U$, we consider $\\gamma :\\mathbb{R}^2 \\to \\mathbb{R}$ defined by $\\gamma(x,y) = x+y$. Then we have\n",
      "$$\\begin{aligned}\n",
      "U \\cap B((x_0,y_0,z_0),1) &= \\{ (x,y,z) \\in B((x_0,y_0,z_0),1) \\mid -z > x  + y\\}\\\\\n",
      "&= \\{ (x,y,z) \\in B((x_0,y_0,z_0),1) \\mid -z > \\gamma(x,y) \\}.\n",
      "\\end{aligned}$$\n",
      "\n",
      "By $\\gamma \\in C^1$, we have $\\partial U$ is $C^1$.\n",
      "\n",
      "$(b)$ *Note.* The $\\gamma$ function needs not to be in $C^p$ of the whole $\\mathbb{R}^{n-1}$, we just require it to be in $C^p$ in a neighborhood of $(x^0_1,\\ldots,x^0_{n-1})$ where $(x^0_1,\\ldots,x^0_{n-1},x^0_n)=x_0 $.\n",
      "\n",
      "We consider $\\gamma :\\mathbb{R} \\to \\mathbb{R}$ defined by $\\gamma(x) = \\sqrt[3]{x^3 -1}$. For $(x_0,y_0) \\in \\partial U$ such that $x_0 \\ne 1$, we have\n",
      "$$\\begin{aligned}\n",
      "U \\cap B((x_0,y_0),1) &= \\{ (x,y) \\in B((x_0,y_0),1) \\mid -y > \\sqrt[3]{x^3 -1} \\}\\\\\n",
      "&= \\{ (x,y) \\in B((x_0,y_0),1) \\mid -y > \\gamma(x) \\}.\n",
      "\\end{aligned}$$\n",
      "\n",
      "For $(x_0,y_0) = (1,0)$, we have\n",
      "$$\\begin{aligned}\n",
      "U \\cap B((x_0,y_0),1) &= \\{ (x,y) \\in B((x_0,y_0),1) \\mid -x > \\sqrt[3]{y^3 -1} \\}\\\\\n",
      "&= \\{ (x,y) \\in B((x_0,y_0),1) \\mid -x>  \\gamma(y) \\}.\n",
      "\\end{aligned}$$\n",
      "\n",
      "By in the first case, $\\gamma \\in C^1$ in a neighborhood of $x_0$ and in the second case, $\\gamma \\in C^1$ in a neighborhood of $y_0$, we have $\\partial U$ is $C^1$. $\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**4.** Show that $dx/dt =\\sqrt{x}$, $x(0) = 0$ has two solutions:\n",
      "$$x(t) = 0 \\quad\\text{and}\\quad x(t) = \\begin{cases} 0, & \\text{if } t\\le 0,\\\\ t^2/4, &\\text{else.}\\end{cases}$$\n",
      "\n",
      "Does this contradict Theorem 6?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** For the case $x(t) = 0$, we have $x(0) = 0$ and $x'(t) = 0 = \\sqrt{x(t)}$. So $x(t)  =0$ is one solution.\n",
      "\n",
      "For the later case, we have $x(0) = 0$ and $x'(t) = 0 = \\sqrt{x(t)}$ if $t < 0$, $x'(t) =t/2 = \\sqrt{x(t)}$ if $t > 0$, and $x'(0) = 0$. So it is another solution.\n",
      "\n",
      "This does not contradict Theorem 6, because the square function $\\sqrt{x}$ is not defined on $[-r, r]$ for all $r > 0$. Moreover, if we consider $\\sqrt{x}$ is $\\sqrt{|x|}$, then still, we have another reason: $\\sqrt{|x|}$ is not Lipschitz in any neighborhood of $0$. Indeed, we have\n",
      "$$|\\sqrt{x} - \\sqrt{y}| = \\frac{|x-y|}{\\sqrt{x} + \\sqrt{y}}$$\n",
      "for all $x,y > 0$ and\n",
      "$$\\lim_{x,y\\to 0^+} \\frac{1}{\\sqrt{x} + \\sqrt{y}} = +\\infty. \\Box$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**5.** Consider the equation $dx/dt = 1 + tx$, $x(0) = 0$. Examine the iteration scheme given in the text to obtain a power series expression for the solution. Examine the radius of convergence."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** Put $f(t,x) = 1 + tx$. Then $x_1(t) = 0$, $x_2(t) = 0 + \\int_0^t f(s,x_1(s))\\,ds = \\int_0^t f(s,0)\\,ds = t$, and \n",
      "$$x_3(t) = 0 + \\int_0^tf(s,x_2(s))\\,ds = \\int_0^t f(s,s)\\,ds = t + t^3/3.$$\n",
      "\n",
      "This suggests that\n",
      "$$\\tag{*} x_n(t) = \\sum_{i = 1}^{n-1} \\frac{t^{2i-1}}{1.3\\ldots  (2i-1)}.$$\n",
      "\n",
      "Indeed, by induction, suppose the above formula is true for $n$. Then\n",
      "$$\\begin{aligned}\n",
      "x_{n+1}(t) &= 0 + \\int_0^t f(s,x_n(s))\\,ds\\\\\n",
      "&= \\int_0^t f\\left(s, \\sum_{i = 1}^{n-1} \\frac{s^{2i-1}}{1.3\\ldots  (2i-1)}\\right) \\, ds\\\\\n",
      "&= \\int_0^t \\left(1 + \\sum_{i=1}^{n-1} \\frac{s^{2i}}{1.3\\ldots (2i-1)} \\right)\\, ds \\\\\n",
      "&= \\sum_{i=0}^{n-1} \\frac{t^{2i+1}}{1.3\\ldots (2i-1)(2i+1)}\\\\\n",
      "& = \\sum_{i=1}^n \\frac{t^{2i-1}}{1.3\\ldots (2i-1)}.\n",
      "\\end{aligned}$$\n",
      "\n",
      "So $(*)$ is true for all $n$. We obtain a power series expression for the solution:\n",
      "$$x(t) = \\sum_{i = 1}^{\\infty} \\frac{t^{2i-1}}{1.3\\ldots (2i-1)}.$$\n",
      "\n",
      "Now, we may write $x(t)$ in the following form:\n",
      "$$x(t) = t\\sum_{i = 1}^{\\infty} \\frac{(t^2)^{i-1}}{1.3\\ldots (2i-1)} = t \\sum_{i = 0}^{\\infty} \\frac{(t^2)^i}{1.3\\ldots (2i+1)}.$$\n",
      "\n",
      "The radius of convergence $R$ of $x(t)$ is the square root of the radius of convergence of the series\n",
      "$$\n",
      "\\sum_{i=0}^{\\infty} \\frac{s^i}{1.3\\ldots (2i+1)}.\n",
      "$$\n",
      "\n",
      "By the ratio test, we have\n",
      "$$\\frac{1}{R^2} = \\lim_{n\\to \\infty} \\frac{{1.3\\ldots  (2n+1)}}{1.3\\ldots  (2n+3)} = \\lim_{n\\to\\infty} \\frac{1}{2n+3}= 0.$$\n",
      "\n",
      "So $R = +\\infty$. $\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**6.** Let $f : \\mathbb{R} \\times \\mathbb{R}^n \\to \\mathbb{R}^n$ be a given continuous mapping. Suppose there is a constant $K$ such that\n",
      "$$\\|f (t, x) \u2212 f (t, y)\\| \u2264 K \\| x \u2212 y\\|$$\n",
      "for all $t \\in \\mathbb{R}$, $x, y \\in \\mathbb{R}^n$. Then there is a unique continuously differentiable map $x : \\mathbb{R} \\to \\mathbb{R}^n$ such that\n",
      "$$\\begin{cases}\n",
      "x(0) = x_0, & (\\text{initial condition}),\\\\\n",
      "dx/dt = f(t,x(t)), & (t,x) \\in \\mathbb{R} \\times \\mathbb{R}^n.\n",
      "\\end{cases}$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** Let $b > 0$ such that $b < 1/K$. Put $A = \\{\\varphi \\in \\mathcal{C}([0,b], \\mathbb{R}^n) \\mid \\varphi(0) = x_0\\}$. By $\\mathcal{C}([0,b], \\mathbb{R}^n)$ is a complete metric space and $A$ is its closed subset (if $\\{\\varphi_n \\}\\subset A$ and $\\varphi_n \\to \\varphi$, then $\\varphi_n(0) \\to \\varphi(0)$, hence $\\varphi(0) = x_0$), we have $A$ is also a complete metric space. Now, let $F :A \\to A$ be defined by\n",
      "$$ F(\\varphi) (t) = x_0 + \\int_0^t f(s, \\varphi(s))\\, ds.$$\n",
      "\n",
      "We have\n",
      "$$\\begin{aligned}\n",
      "\\|F(\\varphi) - F(\\psi)\\| &= \\sup_{0\\le t \\le b} \\|F(\\varphi)(t) - F(\\psi)(t)\\| \\\\\n",
      "&= \\sup_{0\\le t\\le b} \\left\\| \\int_0^t [f(s,\\varphi(s)) - f(s,\\psi(s))]\\,ds \\right\\| \\\\\n",
      "&\\le \\sup_{0\\le t\\le b} \\int_{0}^t \\| f(s,\\varphi(s)) - f(s,\\psi(s)) \\| \\,ds\\\\\n",
      "&\\le \\sup_{0\\le t\\le b}\\int_{0}^t K\\|\\varphi - \\psi\\| \\,ds \\\\\n",
      "&\\le Kb \\|\\varphi -\\psi\\|.\n",
      "\\end{aligned}$$\n",
      "\n",
      "By $Kb < 1$, the contraction mapping principle shows that there exists a unique $y_0 \\in A$ such that $F(y_0) = y_0$. In other words, we have\n",
      "$$ y_0(t) = x_0 + \\int_0^t f(s, y_0(s))\\, ds,$$\n",
      "for all $t\\in [0,b]$. Similarly, there exists a unique $y_1\\in \\mathcal{C}([b,2b],\\mathbb{R}^n)$ such that\n",
      "$$ y_1(t) = y_0(b) + \\int_b^t f(s,y_1(s))\\,ds ,$$\n",
      "for all $t \\in [b,2b]$. By an \"induction\" procedure, we get a unique sequence $\\{y_n\\}$ such that $y_n \\in \\mathcal{C}([nb,(n+1)b],\\mathbb{R}^n)$ and\n",
      "$$y_n(t) = y_{n-1}(nb) + \\int_{nb}^t f(s,y_n(s))\\,ds ,$$\n",
      "for all $t \\in [nb,(n+1)b]$.\n",
      "\n",
      "Similarly, we get a unique sequence $\\{y_{-n}\\}$ such that $y_{-n} \\in \\mathcal{C}([-nb, (-n+1)b],\\mathbb{R}^n)$ and\n",
      "$$y_{-n}(t) = y_{-n+1}((-n+1)b) + \\int_{(-n+1)b}^t f(s,y_{-n}(s))\\,ds,$$\n",
      "for all $t \\in [-nb, (-n+1)b]$.\n",
      "\n",
      "Now, let $x:\\mathbb{R}\\to \\mathbb{R}^n$ be defined by $x(t) = y_n(t)$ if $t \\in [n, n+1)$ for some $n\\in \\mathbb{Z}$. It is clear that $x \\in \\mathcal{C}(\\mathbb{R},\\mathbb{R}^n)$ because $y_n(n+1) = y_{n+1}(n+1)$ for all $n\\in \\mathbb{Z}$. We claim that\n",
      "$$x(t) = x_0 + \\int_0^t f(s, x(s))\\,ds,$$\n",
      "for all $t\\in \\mathbb{R}$. Indeed,let $t\\in \\mathbb{R}$ and suppose that $t \\in [n, n+1)$ for some $n\\ge 1$. We have\n",
      "$$\\begin{aligned}\n",
      "x(t) = y_n(t) &= y_{n-1}(nb) +  \\int_{nb}^t f(s,y_n(s))\\,ds\\\\\n",
      "& = y_{n-1}(nb) + \\int_{nb}^t f(s,x(s)) \\, ds \\\\\n",
      "& = y_{n-2}((n-1)b) + \\int_{(n-1)b}^{nb} f(s,y_{(n-1)b}) \\, ds + \\int_{nb}^t f(s,x(s))\\, ds \\\\\n",
      "& = y_{n-2}((n-1)b) + \\int_{(n-1)b}^t f(s,x(s))\\, ds \\\\\n",
      "& = \\ldots \\\\\n",
      "& = x_0 + \\int_0^t f(s,x(s)) \\,ds.\n",
      "\\end{aligned}$$\n",
      "\n",
      "If $n < 1$, similarly, we get a same result. So far we have prove the existence of a solution for the problem. Now, we prove the uniqueness. Indeed, suppose $z\\in \\mathcal{C}(\\mathbb{R},\\mathbb{R}^n)$ is a solution. Then through an \"induction\" procedure, we can prove that the restriction of $z$ in each interval $[nb,(n+1)b]$ must be $y_n$ (by the uniqueness of $y_n$). So $z = x$. We conclude. $\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**7.** Let $A = (a_{ij})$ be an $n \\times n$ matrix and define $\\|A\\| := \\sup |a_{ij} |$.\n",
      "\n",
      "$(a)$ Let $A, B$ be $n \\times n$ matrices, then $\\|AB \\|\u2264 n\\| A\\| \\|B\\|$.\n",
      "\n",
      "$(b)$ $\\|A^m\\| \u2264 n^{m\u22121}\\| A\\|^m$.\n",
      "\n",
      "$(c)$ Prove that\n",
      "$$e^{A} := \\sum_{m=0}^{\\infty}\\frac{A^m}{m!}$$\n",
      "is well-defined; that is, every component of $B_k =\\sum_{m=k}^{\\infty}\\frac{A^m}{m!}$ converges to $0$ as $k \\to \\infty$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** $(a)$ We have $|[AB]_{ij}| = |\\sum_{k=1}^n A_{ik}B_{kj}| \\le n \\|A\\| \\|B\\|$. Hence $\\|AB \\| \\le n\\|A\\| \\|B\\|$.\n",
      "\n",
      "$(b)$ By an \"induction\" procedure, we have $\\|A^m\\| \\le n \\|A^{m-1}\\| \\|A\\| \\le n\\left(n^{m-2} \\|A\\|^{m-1}\\right)\\|A\\| = n^{m-1}\\|A\\|^m$.\n",
      "\n",
      "$(c)$ We have $\\|\\cdot \\|$ is a norm in the vector space $\\mathcal{M}$ of $n\\times n$ matrices. Also, this norm makes $\\mathcal{M}$ become a complete norm space. Indeed, if $\\{M_k\\}$ is a Cauchy sequence in $\\mathcal{M}$, then for $1\\le i,j\\le n$, we have $\\{[M_k]_{ij}\\}$ also a Cauchy sequence in $\\mathbb{R}$, hence converges to $[M]_{ij}$. Then each component of $(M_k - M)$ converges to $0$ as $k \\to \\infty$. So $\\| M_k - M\\| \\to 0$ as $k\\to \\infty$ by a usual $\\epsilon-\\delta$ argument, hence $M_k \\to M$ as $k \\to \\infty$.\n",
      "\n",
      "Now, we have\n",
      "$$\\left\\|\\frac{A^m}{m!}\\right\\| \\le  \\frac{n^{m-1} \\|A\\|^m}{m!}= \\frac{(n\\|A\\|)^m}{n.m!}.$$\n",
      "\n",
      "By\n",
      "$$\\lim_{m\\to \\infty} \\frac{\\frac{(n\\|A\\|)^{m+1}}{n.(m+1)!}}{\\frac{(n\\|A\\|)^m}{n.m!}} = \\lim_{m\\to\\infty} \\frac{n\\|A\\|}{m+1} = 0,$$\n",
      "we have the series $\\sum_{m=0}^{\\infty}\\frac{(n\\|A\\|)^m}{n.m!}$ converges, hence the series $\\sum_{m=0}^{\\infty}\\frac{A^m}{m!}$ converges by the comparison test (here we use the completeness of $\\mathcal{M}$). So $e^A$ is well-defined.\n",
      "\n",
      "Finally, by $e^A$ is well-defined, we have $\\|B_k\\| \\to 0$ as $k \\to \\infty$, hence each component of $B_k$ converges to $0$ as $k \\to \\infty$. Reversely, if each component of $B_k$ converges to $0$ as $k \\to \\infty$, then $\\|B_k\\| \\to 0$ by a usual $\\epsilon-\\delta$ argument. $\\Box$ "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**8.** Let $A$ be an $n \\times n$ matrix and consider the linear system\n",
      "$$\\frac{dx}{dt}= Ax(t),\\quad x(t) \\in \\mathbb{R}^n.$$\n",
      "\n",
      "Show that a solution is\n",
      "$$x(t) = e^{tA} x(0),\\quad \\text{where }e^B =\\sum_{m=0}^{\\infty}\\frac{B^m}{m!}.$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** It is enough to show that for all $x_0 \\in \\mathbb{R}$,\n",
      "$$\\tag{*}\\lim_{h\\to 0} \\frac{(e^{(t+h)A}-e^{tA})x_0}{h} = Ae^{tA}x_0.$$\n",
      "\n",
      "By Problem 7, we have\n",
      "$$\\lim_{h\\to 0} \\frac{(e^{(t+h)A}-e^{tA})x_0}{h} = \\lim_{h\\to 0} \\left(\\sum_{m=0}^{\\infty}\\frac{[(t+h)^m-t^m]A^m}{m!h}\\right)x_0.$$\n",
      "\n",
      "With $\\|.\\|_2$ is the Euclidean norm, we have \n",
      "$$\\|Mb\\|_2 = \\sqrt{\\sum_{i=1}^n \\left(\\sum_{j=1}^n m_{ij}b_j\\right)^2} \\le \\sqrt{ n\\|M\\|^2 \\left(\\sum_{j=1}^n |b_j|\\right)^2} \\le n \\|M\\| \\|b\\|_2$$\n",
      "for all $M \\in \\mathcal{M}$ and $b\\in \\mathbb{R}^n$. So if $\\|M_n - M\\| \\to 0$ then $\\|M_nb - Mb\\|_2 \\to 0$. This means that we can put $x_0$ inside the series to get\n",
      "$$\\lim_{h\\to 0} \\frac{(e^{(t+h)A}-e^{tA})x_0}{h} = \\lim_{h\\to 0} \\sum_{m=0}^{\\infty}\\frac{[(t+h)^m-t^m]A^mx_0}{m!h}.$$\n",
      "\n",
      "Now the series converges uniformly for $h\\in [-1,1]\\backslash \\{0\\}$ because for each $m\\ge 1$, we have\n",
      "$$\\begin{aligned}\\left\\|\\frac{[(t+h)^m-t^m]A^mx_0}{m!h}\\right\\| &= \\left\\|\\frac{m(t+\\delta h)^{m-1}A^mx_0}{m!}\\right\\| \\quad\\text{for some }\\delta \\in (0,1) \\\\\n",
      "&\\le\\frac{ |t + \\delta h|^{m-1}n\\|A^m\\| \\|x_0\\|_2}{(m-1)!} \\\\\n",
      "&\\le \\frac{(|t|+1)^{m-1}n^m\\|A\\|^m \\|x_0\\|_2}{(m-1)!}.\n",
      "\\end{aligned}$$\n",
      "and the following series converges:\n",
      "$$\\sum_{m=1}^{\\infty}\\frac{(|t|+1)^{m-1}n^m\\|A\\|^m \\|x_0\\|_2}{(m-1)!} = n\\|A\\|\\|x_0\\|_2 e^{(|t|+1)n\\|A\\|}.$$\n",
      "\n",
      "Recall in proof of HW#6, Problem 1, we just need $f_n$ converge to $f$ uniformly in a deleted neighborhood of $x$ for an interchange of limits:\n",
      "$$ \\lim_{t\\to x}\\lim_{n\\to\\infty} f_n(t) = \\lim_{n\\to \\infty}\\lim_{t\\to x}f_n(t).$$\n",
      "\n",
      "So we have\n",
      "$$\\begin{aligned}\n",
      "\\lim_{h\\to 0} \\frac{(e^{(t+h)A}-e^{tA})x_0}{h} &=  \\sum_{m=0}^{\\infty}\\lim_{h\\to 0}\\frac{[(t+h)^m-t^m]A^mx_0}{m!h}\\\\\n",
      "& = \\sum_{m=1}^{\\infty} \\frac{mt^{m-1}A^m x_0}{m!}\\\\\n",
      "& = \\left(\\sum_{m=1}^{\\infty} A\\frac{t^{m-1}A^{m-1}}{(m-1)!}\\right)x_0\\\\\n",
      "& = A e^{tA}x_0.\n",
      "\\end{aligned}$$\n",
      "\n",
      "In the last equality, we can take $A$ outside the series because $Ab_n \\to Ab$ if $b_n \\to b$ in $\\mathbb{R}^n$. We conclude. $\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**9.** Let $f (x, y) = x^2 + y^2 + 3y^3 + 8x^4 + x^2 e^x \\sin x + 6$. Show that there exist new coordinates $\\xi$, $\\eta$, where\n",
      "$$\\xi = \\xi(x, y),\\quad \\eta = \\eta(x, y),$$\n",
      "for which\n",
      "$$f (x, y) = \\xi^2 + \\eta^2 + 6$$\n",
      "in a neighborhood of $(0, 0)$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** We have\n",
      "$$\\frac{\\partial f}{\\partial x} = 2x + 32x^3 + 2xe^x\\sin x + x^2 e^x \\sin x + x^2 e^x \\cos x,\\quad \\frac{\\partial f}{\\partial y} = 2y + 9y^2,$$\n",
      "$$\\frac{\\partial^2 f}{\\partial x^2} = 2 + 96x^2 + 2x^2 e^x \\cos x + 4x e^x \\cos x + 4xe^x \\sin x + 2e^x\\sin x,$$\n",
      "$$\\frac{\\partial^2 f}{\\partial x\\partial y} = \\frac{\\partial^2 f}{\\partial y\\partial x} = 0, \\quad \\frac{\\partial^2 f}{\\partial y^2}= 2 + 18y.$$\n",
      "\n",
      "By $Df(0,0) = 0$, we have $(0,0)$ is a critical point. At $(0,0)$, the Hessian matrix $\\Delta = \\begin{bmatrix}-2 & 0 \\\\ 0 & -2 \\end{bmatrix}$. By $-2 < 0$ and $\\det (\\Delta) = 4 > 0$, $\\Delta$ is negative definite, hence the index of $f$ at $(0,0)$ is $0$. Apply the Morse Lemma, there exist such new coordinates in a neighborhood of $(0,0)$. $\\Box$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**10.** Compute the index of the function $x^2 + y^2 \u2212 7x \u2212 8y + xy + 16 + (x \u2212 2)^3$ at its critical point $x = 2$, $y = 3$. Discuss the nature of the function near this point."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Proof.** Put $f(x,y) = x^2 + y^2 \u2212 7x \u2212 8y + xy + 16 + (x \u2212 2)^3$. We have\n",
      "$$\\frac{\\partial f}{\\partial x} = 2x - 7 + y + 3(x-2)^2,\\quad \\frac{\\partial f}{\\partial y} = 2y - 8 + x,$$\n",
      "$$\\frac{\\partial^2 f}{\\partial x^2} = 2 + 6(x-2),\\quad \\frac{\\partial^2 f}{\\partial x\\partial y} =  \\frac{\\partial^2 f}{\\partial y\\partial x} = 1,\\quad \\frac{\\partial^2 f}{\\partial y^2} = 2.$$\n",
      "\n",
      "By $Df(2,3) = 0$, we have $(2,3)$ is a critical point. At $(2,3)$, the Hessian matrix $\\Delta = \\begin{bmatrix}-2 & -1 \\\\ -1 & -2 \\end{bmatrix}$. By $ -2 < 0$ and $\\det (\\Delta) = 3 > 0$, $\\Delta$ is negative definite, hence the index of $f$ at $(2,3)$ is $0$. Apply the Morse Lemma, near $(2,3)$ the function is approximately a paraboloid and in some new coordinate system it is exactly a paraboloid. $\\Box$"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}