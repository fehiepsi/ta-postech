
% Default to the notebook output style

    


% Inherit from the specified cell style.




    
\documentclass{article}

    
    
    \usepackage{graphicx} % Used to insert images
    \usepackage{adjustbox} % Used to constrain images to a maximum size 
    \usepackage{color} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage[utf8x]{inputenc} % Allow utf-8 characters in the tex document
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range 
    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    

    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=blue,
      linkcolor=darkorange,
      citecolor=darkgreen,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

    \begin{document}
    
    
    
    
    

    
    \subsection*{Analysis I - homework - week
11}\label{analysis-i---homework---week-11}
\addcontentsline{toc}{subsection}{Analysis I - homework - week 11}

    \textbf{1.} $(a)$ Suppose that $f : \mathbb{R}^n \to \mathbb{R}^m$ is of
class $C^1$ and $Df (x_0 )$ has rank $m$. This means that $Df (x_0 )$ as
a linear map is onto. Then show that there is a neighborhood of $f(x_0)$
lying in the image of $f$.

$(b)$ Suppose $f : \mathbb{R}^n \to \mathbb{R}^m$ is $C^1$ and
$Df (x_0 )$ is one-to-one. Show $f$ is one-to-one on a neighborhood of
$x_0$.

    \textbf{Proof.} \emph{Lemma.} (Generalization of Straightening-out
Theorem) Let $f: \mathbb{R}^n \to \mathbb{R}^m$, $n\ge m$, $f \in C^1$.
Suppose $f(x_0) = 0$ and $Df(0)$ has rank $m$. Then there is an open set
$U$, an open set $V$ containing $x_0$, and a function $h:U\to V$ of
class $C^1$, with inverse $h^{-1}:V\to U$ of class $C^1$, such that for
all $(x_1,\ldots,x_n)\in U$, we have
\[f(h(x_1,\ldots,x_n)) = (x_{n-m+1},\ldots,x_n).\]

For a proof of this lemma, see Exercise 7.3 in the textbook and imitate
the proof of Straightening-out Theorem without any much modification
(replace $\mathbb{R}^{n-1}\times \mathbb{R}$ by
$\mathbb{R}^{n-m}\times \mathbb{R}^m$).

$(a)$ By $Df(x_0)$ has rank $m$, we have $n \ge m$. Put
$g(x) = f(x) - f(x_0)$, so $g(x_0) = 0$. Moreover, $Dg(x_0) = Df(x_0)$,
so $Dg(x_0)$ has rank $m$. By the above lemma, there is an open set $U$,
an open set $V$ containing $x_0$, and a surjective function $h:U\to V$
such that for all $(x_1,\ldots,x_n)\in U$, we have
\[g(h(x_1,\ldots,x_n)) = (x_{n-m+1},\ldots,x_n).\]

We have $g(V) = (g\circ h) (U)$, which is open because $g\circ h$ is a
projection. By $x_0 \in V$, we conclude $g(V)$ is a neighborhood of $0$,
hence $f(V)\subset \operatorname{Im}(f)$ is a neighborhood of $f(x_0)$.

$(b)$ In Linear Algebra, we know that if $Df(x_0)$ is one-to-one then
$Df(x_0)$ has rank $n$ (by
$n = \operatorname{rank}Df(x_0) + \dim \operatorname{Ker}( Df(x_0))$).
So $n\le m$. Apply Theorem 7.4, there are open sets $U$ and $V$ in
$\mathbb{R}^m$ with $f(x_0)\in U$ and a function $g:U\to V$ such that
for all $(x_1,\ldots,x_n)\in f^{-1}(U)$, we have
\[g\circ  f(x_1,\ldots,x_n) = (x_1,\ldots,x_n,0,\ldots,0).\]

By $g\circ f$ is one-to-one on $f^{-1}(U)$, $f$ must be one-to-one on
$f^{-1}(U)$, which is a neighborhood of $x_0$. $\Box$

    \textbf{2.} Does the function $h$ in Theorem 3 (\emph{Straightening-out
Theorem}) have to be unique? Discuss.

    \textbf{Proof.} No. For example, let $f:\mathbb{R}^2 \to \mathbb{R}^2$
be defined by $f(x,y) = x+y$. Clearly, $f \in C^1$ (indeed
$f\in C^{\infty}$) and $Df(0,0) \ne 0$. Consider
$h_1,h_2 :\mathbb{R}^2 \to \mathbb{R}^2$ be defined by
$h_1(x,y) = (x,y -x)$, $h_2(x,y) = (x+1,y-x-1)$. We have
$h_1^{-1}(x,y) = (x,x+y)$ and $h_2^{-1}(x,y) = (x-1, x + y )$, so all
$h_1, h_2, h_1^{-1}, h_2^{-1}$ are in $C^{1}$. Moreover,
$f(h_1(x,y)) = f(h_2(x,y)) = y$. We get two functions satisfy the
conclusion of Theorem 3. $\Box$

    \textbf{3.} Let $U \subset \mathbb{R}^n$ be open. We say the boundary
$\partial U$ is $C^p$ if for each point $x_0 \in \partial U$ there exist
$r > 0$ and a $C^p$ function $\gamma : \mathbb{R}^{n-1} \to \mathbb{R}$
such that - upon relabeling and reorienting the coordinates axes if
necessary - we have
\[U \cap B(x_0 , r) = \{x \in B(x_0 , r) \mid x_n > \gamma(x_1 , \ldots , x_{n−1})\}.\]

Likewise, $\partial U$ is $C^{\infty}$ if $\gamma$ is $C^{\infty}$
function.

Determine that the following boundaries are $C^1$:

$(a)$ $U := \{(x, y, z) \in \mathbb{R}^3 \mid x + y + z < 0\}$ and its
boundary is
$\partial U := \{(x, y, z) \in \mathbb{R}^3 \mid x + y + z = 0\}$.

$(b)$ $U := \{(x, y) \in \mathbb{R}^2 \mid  x^3 + y^3 < 1\}$ and its
boundary is
$\partial U := \{(x, y) \in \mathbb{R}^2 \mid  x^3 + y^3 = 1\}$.

    \textbf{Proof.} $(a)$ For $(x_0,y_0,z_0) \in \partial U$, we consider
$\gamma :\mathbb{R}^2 \to \mathbb{R}$ defined by $\gamma(x,y) = x+y$.
Then we have \[\begin{aligned}
U \cap B((x_0,y_0,z_0),1) &= \{ (x,y,z) \in B((x_0,y_0,z_0),1) \mid -z > x  + y\}\\
&= \{ (x,y,z) \in B((x_0,y_0,z_0),1) \mid -z > \gamma(x,y) \}.
\end{aligned}\]

By $\gamma \in C^1$, we have $\partial U$ is $C^1$.

$(b)$ \emph{Note.} The $\gamma$ function needs not to be in $C^p$ of the
whole $\mathbb{R}^{n-1}$, we just require it to be in $C^p$ in a
neighborhood of $(x^0_1,\ldots,x^0_{n-1})$ where
$(x^0_1,\ldots,x^0_{n-1},x^0_n)=x_0 $.

We consider $\gamma :\mathbb{R} \to \mathbb{R}$ defined by
$\gamma(x) = \sqrt[3]{x^3 -1}$. For $(x_0,y_0) \in \partial U$ such that
$x_0 \ne 1$, we have \[\begin{aligned}
U \cap B((x_0,y_0),1) &= \{ (x,y) \in B((x_0,y_0),1) \mid -y > \sqrt[3]{x^3 -1} \}\\
&= \{ (x,y) \in B((x_0,y_0),1) \mid -y > \gamma(x) \}.
\end{aligned}\]

For $(x_0,y_0) = (1,0)$, we have \[\begin{aligned}
U \cap B((x_0,y_0),1) &= \{ (x,y) \in B((x_0,y_0),1) \mid -x > \sqrt[3]{y^3 -1} \}\\
&= \{ (x,y) \in B((x_0,y_0),1) \mid -x>  \gamma(y) \}.
\end{aligned}\]

By in the first case, $\gamma \in C^1$ in a neighborhood of $x_0$ and in
the second case, $\gamma \in C^1$ in a neighborhood of $y_0$, we have
$\partial U$ is $C^1$. $\Box$

    \textbf{4.} Show that $dx/dt =\sqrt{x}$, $x(0) = 0$ has two solutions:
\[x(t) = 0 \quad\text{and}\quad x(t) = \begin{cases} 0, & \text{if } t\le 0,\\ t^2/4, &\text{else.}\end{cases}\]

Does this contradict Theorem 6?

    \textbf{Proof.} For the case $x(t) = 0$, we have $x(0) = 0$ and
$x'(t) = 0 = \sqrt{x(t)}$. So $x(t)  =0$ is one solution.

For the later case, we have $x(0) = 0$ and $x'(t) = 0 = \sqrt{x(t)}$ if
$t < 0$, $x'(t) =t/2 = \sqrt{x(t)}$ if $t > 0$, and $x'(0) = 0$. So it
is another solution.

This does not contradict Theorem 6, because the square function
$\sqrt{x}$ is not defined on $[-r, r]$ for all $r > 0$. Moreover, if we
consider $\sqrt{x}$ is $\sqrt{|x|}$, then still, we have another reason:
$\sqrt{|x|}$ is not Lipschitz in any neighborhood of $0$. Indeed, we
have \[|\sqrt{x} - \sqrt{y}| = \frac{|x-y|}{\sqrt{x} + \sqrt{y}}\] for
all $x,y > 0$ and
\[\lim_{x,y\to 0^+} \frac{1}{\sqrt{x} + \sqrt{y}} = +\infty. \Box\]

    \textbf{5.} Consider the equation $dx/dt = 1 + tx$, $x(0) = 0$. Examine
the iteration scheme given in the text to obtain a power series
expression for the solution. Examine the radius of convergence.

    \textbf{Proof.} Put $f(t,x) = 1 + tx$. Then $x_1(t) = 0$,
$x_2(t) = 0 + \int_0^t f(s,x_1(s))\,ds = \int_0^t f(s,0)\,ds = t$, and
\[x_3(t) = 0 + \int_0^tf(s,x_2(s))\,ds = \int_0^t f(s,s)\,ds = t + t^3/3.\]

This suggests that
\[\tag{*} x_n(t) = \sum_{i = 1}^{n-1} \frac{t^{2i-1}}{1.3\ldots  (2i-1)}.\]

Indeed, by induction, suppose the above formula is true for $n$. Then
\[\begin{aligned}
x_{n+1}(t) &= 0 + \int_0^t f(s,x_n(s))\,ds\\
&= \int_0^t f\left(s, \sum_{i = 1}^{n-1} \frac{s^{2i-1}}{1.3\ldots  (2i-1)}\right) \, ds\\
&= \int_0^t \left(1 + \sum_{i=1}^{n-1} \frac{s^{2i}}{1.3\ldots (2i-1)} \right)\, ds \\
&= \sum_{i=0}^{n-1} \frac{t^{2i+1}}{1.3\ldots (2i-1)(2i+1)}\\
& = \sum_{i=1}^n \frac{t^{2i-1}}{1.3\ldots (2i-1)}.
\end{aligned}\]

So $(*)$ is true for all $n$. We obtain a power series expression for
the solution:
\[x(t) = \sum_{i = 1}^{\infty} \frac{t^{2i-1}}{1.3\ldots (2i-1)}.\]

Now, we may write $x(t)$ in the following form:
\[x(t) = t\sum_{i = 1}^{\infty} \frac{(t^2)^{i-1}}{1.3\ldots (2i-1)} = t \sum_{i = 0}^{\infty} \frac{(t^2)^i}{1.3\ldots (2i+1)}.\]

The radius of convergence $R$ of $x(t)$ is the square root of the radius
of convergence of the series \[
\sum_{i=0}^{\infty} \frac{s^i}{1.3\ldots (2i+1)}.
\]

By the ratio test, we have
\[\frac{1}{R^2} = \lim_{n\to \infty} \frac{{1.3\ldots  (2n+1)}}{1.3\ldots  (2n+3)} = \lim_{n\to\infty} \frac{1}{2n+3}= 0.\]

So $R = +\infty$. $\Box$

    \textbf{6.} Let $f : \mathbb{R} \times \mathbb{R}^n \to \mathbb{R}^n$ be
a given continuous mapping. Suppose there is a constant $K$ such that
\[\|f (t, x) − f (t, y)\| ≤ K \| x − y\|\] for all $t \in \mathbb{R}$,
$x, y \in \mathbb{R}^n$. Then there is a unique continuously
differentiable map $x : \mathbb{R} \to \mathbb{R}^n$ such that
\[\begin{cases}
x(0) = x_0, & (\text{initial condition}),\\
dx/dt = f(t,x(t)), & (t,x) \in \mathbb{R} \times \mathbb{R}^n.
\end{cases}\]

    \textbf{Proof.} Let $b > 0$ such that $b < 1/K$. Put
$A = \{\varphi \in \mathcal{C}([0,b], \mathbb{R}^n) \mid \varphi(0) = x_0\}$.
By $\mathcal{C}([0,b], \mathbb{R}^n)$ is a complete metric space and $A$
is its closed subset (if $\{\varphi_n \}\subset A$ and
$\varphi_n \to \varphi$, then $\varphi_n(0) \to \varphi(0)$, hence
$\varphi(0) = x_0$), we have $A$ is also a complete metric space. Now,
let $F :A \to A$ be defined by
\[ F(\varphi) (t) = x_0 + \int_0^t f(s, \varphi(s))\, ds.\]

We have \[\begin{aligned}
\|F(\varphi) - F(\psi)\| &= \sup_{0\le t \le b} \|F(\varphi)(t) - F(\psi)(t)\| \\
&= \sup_{0\le t\le b} \left\| \int_0^t [f(s,\varphi(s)) - f(s,\psi(s))]\,ds \right\| \\
&\le \sup_{0\le t\le b} \int_{0}^t \| f(s,\varphi(s)) - f(s,\psi(s)) \| \,ds\\
&\le \sup_{0\le t\le b}\int_{0}^t K\|\varphi - \psi\| \,ds \\
&\le Kb \|\varphi -\psi\|.
\end{aligned}\]

By $Kb < 1$, the contraction mapping principle shows that there exists a
unique $y_0 \in A$ such that $F(y_0) = y_0$. In other words, we have
\[ y_0(t) = x_0 + \int_0^t f(s, y_0(s))\, ds,\] for all $t\in [0,b]$.
Similarly, there exists a unique
$y_1\in \mathcal{C}([b,2b],\mathbb{R}^n)$ such that
\[ y_1(t) = y_0(b) + \int_b^t f(s,y_1(s))\,ds ,\] for all
$t \in [b,2b]$. By an ``induction'' procedure, we get a unique sequence
$\{y_n\}$ such that $y_n \in \mathcal{C}([nb,(n+1)b],\mathbb{R}^n)$ and
\[y_n(t) = y_{n-1}(nb) + \int_{nb}^t f(s,y_n(s))\,ds ,\] for all
$t \in [nb,(n+1)b]$.

Similarly, we get a unique sequence $\{y_{-n}\}$ such that
$y_{-n} \in \mathcal{C}([-nb, (-n+1)b],\mathbb{R}^n)$ and
\[y_{-n}(t) = y_{-n+1}((-n+1)b) + \int_{(-n+1)b}^t f(s,y_{-n}(s))\,ds,\]
for all $t \in [-nb, (-n+1)b]$.

Now, let $x:\mathbb{R}\to \mathbb{R}^n$ be defined by $x(t) = y_n(t)$ if
$t \in [n, n+1)$ for some $n\in \mathbb{Z}$. It is clear that
$x \in \mathcal{C}(\mathbb{R},\mathbb{R}^n)$ because
$y_n(n+1) = y_{n+1}(n+1)$ for all $n\in \mathbb{Z}$. We claim that
\[x(t) = x_0 + \int_0^t f(s, x(s))\,ds,\] for all $t\in \mathbb{R}$.
Indeed,let $t\in \mathbb{R}$ and suppose that $t \in [n, n+1)$ for some
$n\ge 1$. We have \[\begin{aligned}
x(t) = y_n(t) &= y_{n-1}(nb) +  \int_{nb}^t f(s,y_n(s))\,ds\\
& = y_{n-1}(nb) + \int_{nb}^t f(s,x(s)) \, ds \\
& = y_{n-2}((n-1)b) + \int_{(n-1)b}^{nb} f(s,y_{(n-1)b}) \, ds + \int_{nb}^t f(s,x(s))\, ds \\
& = y_{n-2}((n-1)b) + \int_{(n-1)b}^t f(s,x(s))\, ds \\
& = \ldots \\
& = x_0 + \int_0^t f(s,x(s)) \,ds.
\end{aligned}\]

If $n < 1$, similarly, we get a same result. So far we have prove the
existence of a solution for the problem. Now, we prove the uniqueness.
Indeed, suppose $z\in \mathcal{C}(\mathbb{R},\mathbb{R}^n)$ is a
solution. Then through an ``induction'' procedure, we can prove that the
restriction of $z$ in each interval $[nb,(n+1)b]$ must be $y_n$ (by the
uniqueness of $y_n$). So $z = x$. We conclude. $\Box$

    \textbf{7.} Let $A = (a_{ij})$ be an $n \times n$ matrix and define
$\|A\| := \sup |a_{ij} |$.

$(a)$ Let $A, B$ be $n \times n$ matrices, then
$\|AB \|≤ n\| A\| \|B\|$.

$(b)$ $\|A^m\| ≤ n^{m−1}\| A\|^m$.

$(c)$ Prove that \[e^{A} := \sum_{m=0}^{\infty}\frac{A^m}{m!}\] is
well-defined; that is, every component of
$B_k =\sum_{m=k}^{\infty}\frac{A^m}{m!}$ converges to $0$ as
$k \to \infty$.

    \textbf{Proof.} $(a)$ We have
$|[AB]_{ij}| = |\sum_{k=1}^n A_{ik}B_{kj}| \le n \|A\| \|B\|$. Hence
$\|AB \| \le n\|A\| \|B\|$.

$(b)$ By an ``induction'' procedure, we have
$\|A^m\| \le n \|A^{m-1}\| \|A\| \le n\left(n^{m-2} \|A\|^{m-1}\right)\|A\| = n^{m-1}\|A\|^m$.

$(c)$ We have $\|\cdot \|$ is a norm in the vector space $\mathcal{M}$
of $n\times n$ matrices. Also, this norm makes $\mathcal{M}$ become a
complete norm space. Indeed, if $\{M_k\}$ is a Cauchy sequence in
$\mathcal{M}$, then for $1\le i,j\le n$, we have $\{[M_k]_{ij}\}$ also a
Cauchy sequence in $\mathbb{R}$, hence converges to $[M]_{ij}$. Then
each component of $(M_k - M)$ converges to $0$ as $k \to \infty$. So
$\| M_k - M\| \to 0$ as $k\to \infty$ by a usual $\epsilon-\delta$
argument, hence $M_k \to M$ as $k \to \infty$.

Now, we have
\[\left\|\frac{A^m}{m!}\right\| \le  \frac{n^{m-1} \|A\|^m}{m!}= \frac{(n\|A\|)^m}{n.m!}.\]

By
\[\lim_{m\to \infty} \frac{\frac{(n\|A\|)^{m+1}}{n.(m+1)!}}{\frac{(n\|A\|)^m}{n.m!}} = \lim_{m\to\infty} \frac{n\|A\|}{m+1} = 0,\]
we have the series $\sum_{m=0}^{\infty}\frac{(n\|A\|)^m}{n.m!}$
converges, hence the series $\sum_{m=0}^{\infty}\frac{A^m}{m!}$
converges by the comparison test (here we use the completeness of
$\mathcal{M}$). So $e^A$ is well-defined.

Finally, by $e^A$ is well-defined, we have $\|B_k\| \to 0$ as
$k \to \infty$, hence each component of $B_k$ converges to $0$ as
$k \to \infty$. Reversely, if each component of $B_k$ converges to $0$
as $k \to \infty$, then $\|B_k\| \to 0$ by a usual $\epsilon-\delta$
argument. $\Box$

    \textbf{8.} Let $A$ be an $n \times n$ matrix and consider the linear
system \[\frac{dx}{dt}= Ax(t),\quad x(t) \in \mathbb{R}^n.\]

Show that a solution is
\[x(t) = e^{tA} x(0),\quad \text{where }e^B =\sum_{m=0}^{\infty}\frac{B^m}{m!}.\]

    \textbf{Proof.} It is enough to show that for all $x_0 \in \mathbb{R}$,
\[\tag{*}\lim_{h\to 0} \frac{(e^{(t+h)A}-e^{tA})x_0}{h} = Ae^{tA}x_0.\]

By Problem 7, we have
\[\lim_{h\to 0} \frac{(e^{(t+h)A}-e^{tA})x_0}{h} = \lim_{h\to 0} \left(\sum_{m=0}^{\infty}\frac{[(t+h)^m-t^m]A^m}{m!h}\right)x_0.\]

With $\|.\|_2$ is the Euclidean norm, we have
\[\|Mb\|_2 = \sqrt{\sum_{i=1}^n \left(\sum_{j=1}^n m_{ij}b_j\right)^2} \le \sqrt{ n\|M\|^2 \left(\sum_{j=1}^n |b_j|\right)^2} \le n \|M\| \|b\|_2\]
for all $M \in \mathcal{M}$ and $b\in \mathbb{R}^n$. So if
$\|M_n - M\| \to 0$ then $\|M_nb - Mb\|_2 \to 0$. This means that we can
put $x_0$ inside the series to get
\[\lim_{h\to 0} \frac{(e^{(t+h)A}-e^{tA})x_0}{h} = \lim_{h\to 0} \sum_{m=0}^{\infty}\frac{[(t+h)^m-t^m]A^mx_0}{m!h}.\]

Now the series converges uniformly for $h\in [-1,1]\backslash \{0\}$
because for each $m\ge 1$, we have
\[\begin{aligned}\left\|\frac{[(t+h)^m-t^m]A^mx_0}{m!h}\right\| &= \left\|\frac{m(t+\delta h)^{m-1}A^mx_0}{m!}\right\| \quad\text{for some }\delta \in (0,1) \\
&\le\frac{ |t + \delta h|^{m-1}n\|A^m\| \|x_0\|_2}{(m-1)!} \\
&\le \frac{(|t|+1)^{m-1}n^m\|A\|^m \|x_0\|_2}{(m-1)!}.
\end{aligned}\] and the following series converges:
\[\sum_{m=1}^{\infty}\frac{(|t|+1)^{m-1}n^m\|A\|^m \|x_0\|_2}{(m-1)!} = n\|A\|\|x_0\|_2 e^{(|t|+1)n\|A\|}.\]

Recall in proof of HW\#6, Problem 1, we just need $f_n$ converge to $f$
uniformly in a deleted neighborhood of $x$ for an interchange of limits:
\[ \lim_{t\to x}\lim_{n\to\infty} f_n(t) = \lim_{n\to \infty}\lim_{t\to x}f_n(t).\]

So we have \[\begin{aligned}
\lim_{h\to 0} \frac{(e^{(t+h)A}-e^{tA})x_0}{h} &=  \sum_{m=0}^{\infty}\lim_{h\to 0}\frac{[(t+h)^m-t^m]A^mx_0}{m!h}\\
& = \sum_{m=1}^{\infty} \frac{mt^{m-1}A^m x_0}{m!}\\
& = \left(\sum_{m=1}^{\infty} A\frac{t^{m-1}A^{m-1}}{(m-1)!}\right)x_0\\
& = A e^{tA}x_0.
\end{aligned}\]

In the last equality, we can take $A$ outside the series because
$Ab_n \to Ab$ if $b_n \to b$ in $\mathbb{R}^n$. We conclude. $\Box$

    \textbf{9.} Let
$f (x, y) = x^2 + y^2 + 3y^3 + 8x^4 + x^2 e^x \sin x + 6$. Show that
there exist new coordinates $\xi$, $\eta$, where
\[\xi = \xi(x, y),\quad \eta = \eta(x, y),\] for which
\[f (x, y) = \xi^2 + \eta^2 + 6\] in a neighborhood of $(0, 0)$.

    \textbf{Proof.} We have
\[\frac{\partial f}{\partial x} = 2x + 32x^3 + 2xe^x\sin x + x^2 e^x \sin x + x^2 e^x \cos x,\quad \frac{\partial f}{\partial y} = 2y + 9y^2,\]
\[\frac{\partial^2 f}{\partial x^2} = 2 + 96x^2 + 2x^2 e^x \cos x + 4x e^x \cos x + 4xe^x \sin x + 2e^x\sin x,\]
\[\frac{\partial^2 f}{\partial x\partial y} = \frac{\partial^2 f}{\partial y\partial x} = 0, \quad \frac{\partial^2 f}{\partial y^2}= 2 + 18y.\]

By $Df(0,0) = 0$, we have $(0,0)$ is a critical point. At $(0,0)$, the
Hessian matrix $\Delta = \begin{bmatrix}-2 & 0 \\ 0 & -2 \end{bmatrix}$.
By $-2 < 0$ and $\det (\Delta) = 4 > 0$, $\Delta$ is negative definite,
hence the index of $f$ at $(0,0)$ is $0$. Apply the Morse Lemma, there
exist such new coordinates in a neighborhood of $(0,0)$. $\Box$

    \textbf{10.} Compute the index of the function
$x^2 + y^2 − 7x − 8y + xy + 16 + (x − 2)^3$ at its critical point
$x = 2$, $y = 3$. Discuss the nature of the function near this point.

    \textbf{Proof.} Put
$f(x,y) = x^2 + y^2 − 7x − 8y + xy + 16 + (x − 2)^3$. We have
\[\frac{\partial f}{\partial x} = 2x - 7 + y + 3(x-2)^2,\quad \frac{\partial f}{\partial y} = 2y - 8 + x,\]
\[\frac{\partial^2 f}{\partial x^2} = 2 + 6(x-2),\quad \frac{\partial^2 f}{\partial x\partial y} =  \frac{\partial^2 f}{\partial y\partial x} = 1,\quad \frac{\partial^2 f}{\partial y^2} = 2.\]

By $Df(2,3) = 0$, we have $(2,3)$ is a critical point. At $(2,3)$, the
Hessian matrix $\Delta = \begin{bmatrix}-2 & 1 \\ 1 & -2 \end{bmatrix}$.
By \$ -2 \textless{} 0\$ and $\det (\Delta) = 3 > 0$, $\Delta$ is
negative definite, hence the index of $f$ at $(2,3)$ is $0$. Apply the
Morse Lemma, near $(2,3)$ the function is approximately a paraboloid and
in some new coordinate system it is exactly a paraboloid. $\Box$


    % Add a bibliography block to the postdoc
    
    
    
    \end{document}
